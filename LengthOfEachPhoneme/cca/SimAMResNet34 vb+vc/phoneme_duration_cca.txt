CCA Similarity: Model Activations vs Phoneme Duration
============================================================

Duration: short
----------------------------------------
first relu: 0.000677
layer1 relu: 0.000444
SimAM: 0.000345
layer1 relu: 0.000345
layer1 relu: 0.000558
SimAM: 0.000510
layer1 relu: 0.000510
layer1 relu: 0.000528
SimAM: 0.000388
layer1 relu: 0.000388
layer2 relu: 0.000821
SimAM: 0.000348
layer2 relu: 0.000348
layer2 relu: 0.000617
SimAM: 0.000478
layer2 relu: 0.000478
layer2 relu: 0.000558
SimAM: 0.000576
layer2 relu: 0.000576
layer2 relu: 0.000827
SimAM: 0.000592
layer2 relu: 0.000592
layer3 relu: 0.001806
SimAM: 0.000765
layer3 relu: 0.000765
layer3 relu: 0.000890
SimAM: 0.000720
layer3 relu: 0.000720
layer3 relu: 0.001012
SimAM: 0.001110
layer3 relu: 0.001110
layer3 relu: 0.001196
SimAM: 0.001365
layer3 relu: 0.001365
layer3 relu: 0.001475
SimAM: 0.001593
layer3 relu: 0.001593
layer3 relu: 0.001907
SimAM: 0.001518
layer3 relu: 0.001518
layer4 relu: 0.002942
SimAM: 0.001841
layer4 relu: 0.001841
layer4 relu: 0.001277
SimAM: 0.001709
layer4 relu: 0.001709
layer4 relu: 0.002042
SimAM: 0.001241
layer4 relu: 0.001241
pooling: 0.005802

Duration: medium
----------------------------------------
first relu: 0.000443
layer1 relu: 0.000610
SimAM: 0.000314
layer1 relu: 0.000314
layer1 relu: 0.000623
SimAM: 0.000297
layer1 relu: 0.000297
layer1 relu: 0.000508
SimAM: 0.000267
layer1 relu: 0.000267
layer2 relu: 0.000320
SimAM: 0.000256
layer2 relu: 0.000256
layer2 relu: 0.000580
SimAM: 0.000124
layer2 relu: 0.000124
layer2 relu: 0.000432
SimAM: 0.000432
layer2 relu: 0.000432
layer2 relu: 0.000585
SimAM: 0.000131
layer2 relu: 0.000131
layer3 relu: 0.000536
SimAM: 0.000798
layer3 relu: 0.000798
layer3 relu: 0.000576
SimAM: 0.000691
layer3 relu: 0.000691
layer3 relu: 0.000182
SimAM: 0.000653
layer3 relu: 0.000653
layer3 relu: 0.000344
SimAM: 0.000605
layer3 relu: 0.000605
layer3 relu: 0.001194
SimAM: 0.000198
layer3 relu: 0.000198
layer3 relu: 0.000090
SimAM: 0.000632
layer3 relu: 0.000632
layer4 relu: 0.001981
SimAM: 0.001991
layer4 relu: 0.001991
layer4 relu: 0.001257
SimAM: 0.000578
layer4 relu: 0.000578
layer4 relu: 0.000373
SimAM: 0.001402
layer4 relu: 0.001402
pooling: 0.005068

