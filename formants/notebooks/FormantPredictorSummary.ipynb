{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886ac7be-4314-4788-8c12-c71b04cf30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea441b0-cb88-45d9-92a4-f005356619da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: C:\\Users\\–ò–ª—å—è\\Desktop\\interp_dev\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/–ò–ª—å—è/Desktop/interp_dev\")  #—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Ä—É—á–Ω—É—é\n",
    "print(\"Working dir:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ac66bb-d33c-4725-8f06-bd97c950c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from formants.transformer.formant_predictor import FormantPredictor\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "hidden_dim = 512\n",
    "num_formants = 3\n",
    "vocab_size = 71\n",
    "pad_token_id = 0\n",
    "\n",
    "model = FormantPredictor(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_formants=num_formants,\n",
    "    pad_token_id=pad_token_id,\n",
    "    max_len=256,\n",
    "    dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43aa9452-9892-4d4d-b678-bac0685f66bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Output shape: torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  \n",
    "\n",
    "speech_embedding = torch.randn(batch_size, 256)  \n",
    "\n",
    "output = model(token_ids, speech_embedding)  \n",
    "print(\"‚úÖ Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c90869eb-5403-455d-b4d8-6effc950b7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Trainable\n",
       "=================================================================================================================================================\n",
       "FormantPredictor                              [2, 5]                    [2, 5, 3]                 --                        True\n",
       "‚îú‚îÄTransformerEncoderAdaLN: 1-1                [2, 5]                    [2, 5, 512]               --                        True\n",
       "‚îÇ    ‚îî‚îÄEmbedding: 2-1                         [2, 5]                    [2, 5, 512]               36,352                    True\n",
       "‚îÇ    ‚îî‚îÄLearnablePositionalEncoding: 2-2       [2, 5, 512]               [2, 5, 512]               --                        True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                    [1, 5]                    [1, 5, 512]               131,072                   True\n",
       "‚îÇ    ‚îî‚îÄDropout: 2-3                           [2, 5, 512]               [2, 5, 512]               --                        --\n",
       "‚îÇ    ‚îî‚îÄMlpAdaLN: 2-4                          [2, 256]                  [2, 512]                  --                        True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-2                       [2, 256]                  [2, 128]                  32,896                    True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-3                         [2, 128]                  [2, 128]                  --                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-4                       [2, 128]                  [2, 3072]                 396,288                   True\n",
       "‚îÇ    ‚îî‚îÄTransformerBlockAdaLN: 2-5             [2, 5, 512]               [2, 5, 512]               --                        True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄAdaptiveLayerNorm: 3-5            [2, 5, 512]               [2, 5, 512]               --                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiHeadSelfAttention: 3-6       [2, 5, 512]               [2, 5, 512]               1,050,624                 True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄAdaptiveLayerNorm: 3-7            [2, 5, 512]               [2, 5, 512]               --                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardAda: 3-8               [2, 5, 512]               [2, 5, 512]               2,099,712                 True\n",
       "‚îú‚îÄLinear: 1-2                                 [2, 5, 512]               [2, 5, 3]                 1,539                     True\n",
       "=================================================================================================================================================\n",
       "Total params: 3,748,483\n",
       "Trainable params: 3,748,483\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 7.37\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.48\n",
       "Params size (MB): 14.99\n",
       "Estimated Total Size (MB): 15.48\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    model,\n",
    "    input_data=(token_ids, speech_embedding),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    depth=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9226259-d068-4fbd-b510-9d6ec5244092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¶–µ–ª–µ–≤—ã–µ —Ñ–æ—Ä–º–∞–Ω—Ç—ã (—Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ)\n",
    "target_formants = torch.randn(batch_size, seq_len, num_formants)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7494af0-dcd1-444b-a042-04533ee09ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Dummy loss: 1.825307011604309\n",
      "‚úÖ Backpropagation and update completed.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "output = model(token_ids, speech_embedding)\n",
    "\n",
    "# –ü–æ—Ç–µ—Ä–∏\n",
    "loss = loss_fn(output, target_formants)\n",
    "print(\"üîÅ Dummy loss:\", loss.item())\n",
    "\n",
    "# –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –∏ —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"‚úÖ Backpropagation and update completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61509d-e6f7-45e9-bf32-b1eab58c3c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
