{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11409c37-7958-48a5-b35b-30d62c144c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PYTHONPATH set to: C:\\Users\\Илья\\Desktop\\interp_dev\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\" PYTHONPATH set to:\", project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b87825da-559c-463c-a0f8-9887fbdd46ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: C:\\Users\\Илья\\Desktop\\interp_dev\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/Илья/Desktop/interp_dev\")  #установка вручную\n",
    "print(\"Working dir:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db7ce13-75fd-4aa9-b423-92b95043f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from formants.transformer.layer_norm_ada import AdaptiveLayerNorm\n",
    "from formants.transformer.multihead_self_attention_ada import MultiHeadSelfAttention\n",
    "from formants.transformer.feed_forward_ada import FeedForwardAda\n",
    "from formants.transformer.transformer_Block_AdaLN import TransformerBlockAdaLN\n",
    "from formants.transformer.mlp_adaln import MlpAdaLN\n",
    "from formants.tokenizer.phoneme_tokenizer import PhonemeTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069dd90-ffbe-497b-a606-f5eed191608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ДАННЫЙ НОУТБУК ВОЗМОЖНО ТРЕБУЕТ ПЕРЕРАБОТКИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2d2bc4-3805-4e0a-940a-b7ea7579e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 5, 512)  \n",
    "alpha = torch.ones(1, 1, 512)\n",
    "beta = torch.zeros(1, 1, 512)\n",
    "gamma = torch.ones(1, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c9154c5-be6d-44e5-8cbb-b66eb702cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaLN out shape: torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "ln = AdaptiveLayerNorm(512)\n",
    "out_ln = ln(x, alpha, beta)\n",
    "print(\"AdaLN out shape:\", out_ln.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57cda925-a5c2-405e-be35-d62f463acd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention out shape: torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "attn = MultiHeadSelfAttention(512, num_heads=8)\n",
    "out_attn = attn(x, gamma)\n",
    "print(\"Attention out shape:\", out_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d01105-39c2-4a7d-8cc0-0387fbb5a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF out shape: torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "ff = FeedForwardAda(512, d_ff=1024, dropout=0.1)\n",
    "out_ff = ff(x, gamma)\n",
    "print(\"FF out shape:\", out_ff.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "471c22cc-08e8-4af7-bddb-10d1728d4954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaLN params shapes:\n",
      "torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "speech_embedding = torch.randn(1, 256)\n",
    "\n",
    "mlp = MlpAdaLN(input_dim=256, hidden_dim=hidden_dim)\n",
    "alpha1, beta1, gamma1, alpha2, beta2, gamma2 = mlp(speech_embedding)\n",
    "\n",
    "print(\"AdaLN params shapes:\")\n",
    "print(alpha1.shape, beta1.shape, gamma1.shape, alpha2.shape, beta2.shape, gamma2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56c308fe-d12e-4980-a9e7-07aa80fe85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block output (random input): torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(1, 5, hidden_dim)\n",
    "block = TransformerBlockAdaLN(hidden_dim=hidden_dim, dropout=0.1)\n",
    "\n",
    "alpha1_ = alpha1.unsqueeze(1).expand_as(x1)\n",
    "beta1_  = beta1.unsqueeze(1).expand_as(x1)\n",
    "gamma1_ = gamma1.unsqueeze(1).expand_as(x1)\n",
    "alpha2_ = alpha2.unsqueeze(1).expand_as(x1)\n",
    "beta2_  = beta2.unsqueeze(1).expand_as(x1)\n",
    "gamma2_ = gamma2.unsqueeze(1).expand_as(x1)\n",
    "\n",
    "out1 = block(x1, alpha1_, beta1_, gamma1_, alpha2_, beta2_, gamma2_)\n",
    "print(\"Block output (random input):\", out1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2016df8-775d-4456-9f23-ebde5d1176b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [35, 8, 44, 49, 67]\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"formants/tokenizer/phoneme_vocab.json\"\n",
    "tokenizer = PhonemeTokenizer(vocab_path)\n",
    "\n",
    "phonemes = ['HH', 'AH0', 'L', 'OW1', 'W']\n",
    "token_ids = tokenizer.encode(phonemes)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "tokens_tensor = torch.tensor(token_ids).unsqueeze(0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a291835b-c3c8-4d61-a9c0-30431945383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block output (token input): torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "embedding = torch.nn.Embedding(num_embeddings=len(tokenizer.vocab), embedding_dim=hidden_dim)\n",
    "x2 = embedding(tokens_tensor) \n",
    "\n",
    "alpha1_ = alpha1.unsqueeze(1).expand_as(x2)\n",
    "beta1_  = beta1.unsqueeze(1).expand_as(x2)\n",
    "gamma1_ = gamma1.unsqueeze(1).expand_as(x2)\n",
    "alpha2_ = alpha2.unsqueeze(1).expand_as(x2)\n",
    "beta2_  = beta2.unsqueeze(1).expand_as(x2)\n",
    "gamma2_ = gamma2.unsqueeze(1).expand_as(x2)\n",
    "\n",
    "out2 = block(x2, alpha1_, beta1_, gamma1_, alpha2_, beta2_, gamma2_)\n",
    "print(\"Block output (token input):\", out2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c85e0ca-bf8c-4382-9bc3-a8c0356663e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "TransformerBlockAdaLN                    [1, 5, 512]               [1, 5, 512]               --\n",
       "├─AdaptiveLayerNorm: 1-1                 [1, 5, 512]               [1, 5, 512]               --\n",
       "├─MultiHeadSelfAttention: 1-2            [1, 5, 512]               [1, 5, 512]               --\n",
       "│    └─Linear: 2-1                       [1, 5, 512]               [1, 5, 1536]              787,968\n",
       "│    └─Dropout: 2-2                      [1, 8, 5, 5]              [1, 8, 5, 5]              --\n",
       "│    └─Linear: 2-3                       [1, 5, 512]               [1, 5, 512]               262,656\n",
       "├─AdaptiveLayerNorm: 1-3                 [1, 5, 512]               [1, 5, 512]               --\n",
       "├─FeedForwardAda: 1-4                    [1, 5, 512]               [1, 5, 512]               --\n",
       "│    └─Linear: 2-4                       [1, 5, 512]               [1, 5, 2048]              1,050,624\n",
       "│    └─Dropout: 2-5                      [1, 5, 2048]              [1, 5, 2048]              --\n",
       "│    └─Linear: 2-6                       [1, 5, 2048]              [1, 5, 512]               1,049,088\n",
       "===================================================================================================================\n",
       "Total params: 3,150,336\n",
       "Trainable params: 3,150,336\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 3.15\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.08\n",
       "Forward/backward pass size (MB): 0.18\n",
       "Params size (MB): 12.60\n",
       "Estimated Total Size (MB): 12.87\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    block,\n",
    "    input_data=(x2, alpha1_, beta1_, gamma1_, alpha2_, beta2_, gamma2_),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea67a3c-5318-44cb-b023-4966a260dc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
