{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gsb9THgaIdV8"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Interp - dev/chroma_db.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('extracted_files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OetDhh7RHGg8",
        "outputId": "9a2115e8-2e64-4c99-86bf-d6eb16c7e490"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fp1CgsBGGs8t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import chromadb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
        "    f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2hVH9gqwHfAu"
      },
      "outputs": [],
      "source": [
        "class EmbeddingsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for loading embeddings\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            source_path,\n",
        "            split,\n",
        "            source_type,\n",
        "            collection_name=\"gender_embeddings\"):\n",
        "        self.lb = LabelEncoder()\n",
        "\n",
        "        if source_type == \"npy\":\n",
        "            self.embeddings, self.labels = self.get_npy_embeddings(\n",
        "                source_path, split)\n",
        "        elif source_type == \"chromadb\":\n",
        "            self.embeddings, self.labels = self.get_chroma_embeddings(\n",
        "                source_path, split, collection_name)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Invalid source type: {source_type}. \"\n",
        "                \"Choose 'npy' or 'chromadb'.\"\n",
        "            )\n",
        "\n",
        "        self.embeddings = torch.tensor(self.embeddings, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
        "\n",
        "    def get_npy_embeddings(self, source_path, split):\n",
        "        \"\"\"\n",
        "        Reads embddings from a .npy file\n",
        "        \"\"\"\n",
        "        source = np.load(os.path.join(\n",
        "            source_path, \"numpy_embs.npy\"), allow_pickle=True)\n",
        "        source = source[0]\n",
        "\n",
        "        if split == \"train\":\n",
        "            embeddings = np.array([item['embedding']\n",
        "                                  for item in source['train']])\n",
        "            labels = [item['label'] for item in source['train']]\n",
        "        elif split == \"test\":\n",
        "            embeddings = np.array([item['embedding']\n",
        "                                  for item in source['test']])\n",
        "            labels = [item['label'] for item in source['test']]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Invalid split. Expected 'test' or 'train', got {split}\")\n",
        "        labels = self.lb.fit_transform(labels)\n",
        "        return embeddings, labels\n",
        "\n",
        "    def get_chroma_embeddings(\n",
        "            self,\n",
        "            source_path,\n",
        "            split,\n",
        "            collection_name=\"gender_embeddings\"):\n",
        "        \"\"\"\n",
        "        Reads embeddings from ChromaDB\n",
        "        \"\"\"\n",
        "        client = chromadb.PersistentClient(path=source_path)\n",
        "        collection = client.get_collection(name=collection_name)\n",
        "        results = collection.get(where={\"split\": split}, include=[\n",
        "            \"embeddings\", \"metadatas\"])\n",
        "        embeddings = np.array(results['embeddings'], dtype=np.float32)\n",
        "        labels = [item['label'] for item in results['metadatas']]\n",
        "\n",
        "        labels = self.lb.fit_transform(labels)\n",
        "        return embeddings, labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltWsZxgdLUTe"
      },
      "source": [
        "Прописываем код для инициализации модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "an9guDSTHsbu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VolumeLevelModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Model class for volume level classification with 2 layers\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=256, hidden_dim=128, num_classes=3):\n",
        "        super(VolumeLevelModel, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.fc1(x))\n",
        "        x1 = self.dropout(x1)\n",
        "        x2 = self.fc2(x1)\n",
        "\n",
        "        return x1, x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DiE1xarQKqhg"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, criterion, num_epoch, device):\n",
        "    \"\"\"\n",
        "    Train a model on a train dataset\n",
        "    \"\"\"\n",
        "    for epoch in tqdm(range(num_epoch), desc=\"Training Progress\"):\n",
        "        model.train()\n",
        "\n",
        "        for embeddings_batch, labels_batch in train_loader:\n",
        "            embeddings_batch = embeddings_batch.to(device)\n",
        "\n",
        "            labels_batch = labels_batch.long()\n",
        "            _, outputs = model(embeddings_batch)\n",
        "            loss = criterion(outputs, labels_batch.to(device))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates a model on a test dataset. Calculates accuracy,\n",
        "    precision, recall and f1-score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_samples_test = 0\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    with torch.no_grad():\n",
        "        for embeddings_batch, labels_batch in tqdm(\n",
        "                test_loader, desc=\"Evaluation Progress\"):\n",
        "            embeddings_batch = embeddings_batch.to(device)\n",
        "\n",
        "            labels_batch = labels_batch.long()\n",
        "            x1, outputs = model(embeddings_batch)\n",
        "\n",
        "            total_samples_test += 1\n",
        "\n",
        "            _, predicted = torch.max(outputs.cpu(), 1)\n",
        "            true_labels.extend(labels_batch.numpy())\n",
        "            pred_labels.extend(predicted.numpy())\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def get_loaders(source_path, source_type):\n",
        "    \"\"\"\n",
        "    Creates dataloaders for train and test files\n",
        "    \"\"\"\n",
        "    train_dataset = EmbeddingsDataset(\n",
        "        source_path, split=\"train\", source_type=source_type)\n",
        "    test_dataset = EmbeddingsDataset(\n",
        "        source_path, split=\"test\", source_type=source_type)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return (\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        test_dataset,\n",
        "        train_dataset.embeddings.shape[1]\n",
        "    )\n",
        "\n",
        "\n",
        "def save_visualization(model, vectors, labels, save_path, device):\n",
        "    \"\"\"\n",
        "    Saves embedding visualization in .png files\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    vectors = torch.FloatTensor(vectors).to(device)\n",
        "    print(vectors.shape)\n",
        "    with torch.no_grad():\n",
        "        x1, predicted = model(vectors)\n",
        "        print(\"Число образцов (строк):\", x1.shape)\n",
        "        print(\"Число признаков (столбцов):\", x1.shape[1])\n",
        "\n",
        "    reducer = TSNE(n_components=2, random_state=42)\n",
        "    x1_reduced = reducer.fit_transform(x1.detach().cpu().numpy())\n",
        "\n",
        "    unique_labels = list(set(labels))\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for label in unique_labels:\n",
        "        indices = [i for i, lbl in enumerate(labels) if lbl == label]\n",
        "        plt.scatter(\n",
        "            x1_reduced[indices, 0],\n",
        "            x1_reduced[indices, 1],\n",
        "            label=f\"Label: {label}\",\n",
        "            alpha=0.6\n",
        "        )\n",
        "\n",
        "    plt.title(\"Visualization of embeddings after first layer\")\n",
        "    plt.legend()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_metrics(metrics, save_path):\n",
        "    \"\"\"\n",
        "    Saves computed metrics in .txt file\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    with open(save_path, 'w') as f:\n",
        "        for key, value in metrics.items():\n",
        "            f.write(f\"{key}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-KyCdJMM597",
        "outputId": "e814290f-8930-4a85-efd8-c0c40a1a84ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 200/200 [00:23<00:00,  8.53it/s]\n",
            "Evaluation Progress: 100%|██████████| 10/10 [00:00<00:00, 1152.31it/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loader, test_loader, test_dataset, input_dim = get_loaders(\n",
        "    '/content/extracted_files/chroma_db', 'chromadb'\n",
        ")\n",
        "model = VolumeLevelModel(input_dim, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "train(model, train_loader, optimizer,\n",
        "      criterion, num_epoch=200, device=device)\n",
        "\n",
        "metrics = evaluate(model, test_loader, device)\n",
        "save_metrics(metrics, '/content/result/VolumeLevel.txt')\n",
        "save_visualization(\n",
        "    model, test_dataset.embeddings.numpy(),\n",
        "    test_dataset.labels.numpy(), '/content/result/VolumeLevel.png', device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yRPwGkzp9y-0"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"/content/VolumeLevelModel/full_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5SwTZxq-mm9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
