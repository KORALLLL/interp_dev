average
first relu: 0.02935209678524786
layer1 relu: 0.03142799302207388
SimAM: 0.030575249396000287
layer1 relu: 0.030575249396000287
layer1 relu: 0.029503536074787153
SimAM: 0.02996622939968995
layer1 relu: 0.02996622939968995
layer1 relu: 0.028925331189970174
SimAM: 0.029208184124374956
layer1 relu: 0.029208184124374956
layer2 relu: 0.03788318064017832
SimAM: 0.04035177093995705
layer2 relu: 0.04035177093995705
layer2 relu: 0.038720450925582676
SimAM: 0.039995488049172565
layer2 relu: 0.039995488049172565
layer2 relu: 0.03699314268484352
SimAM: 0.03851006337893317
layer2 relu: 0.03851006337893317
layer2 relu: 0.037851584060135834
SimAM: 0.0384744198610844
layer2 relu: 0.0384744198610844
layer3 relu: 0.05398379439044438
SimAM: 0.05305778464906695
layer3 relu: 0.05305778464906695
layer3 relu: 0.05477663846940961
SimAM: 0.05597066237413306
layer3 relu: 0.05597066237413306
layer3 relu: 0.05322725160608123
SimAM: 0.05321446989487086
layer3 relu: 0.05321446989487086
layer3 relu: 0.055914286851889865
SimAM: 0.055813867052577244
layer3 relu: 0.055813867052577244
layer3 relu: 0.05281816650809187
SimAM: 0.055905320466374905
layer3 relu: 0.055905320466374905
layer3 relu: 0.0531624397883376
SimAM: 0.05536507123470466
layer3 relu: 0.05536507123470466
layer4 relu: 0.08465115921606134
SimAM: 0.07518866370535572
layer4 relu: 0.07518866370535572
layer4 relu: 0.07506708985637368
SimAM: 0.07605124861328494
layer4 relu: 0.07605124861328494
layer4 relu: 0.08117294276625794
SimAM: 0.07949522369341795
layer4 relu: 0.07949522369341795
pooling: 0.3256820188309673
