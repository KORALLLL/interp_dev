average
first relu: 0.029479708673914175
layer1 relu: 0.03190794892860885
SimAM: 0.030861940769922894
layer1 relu: 0.030861940769922894
layer1 relu: 0.02983192653744959
SimAM: 0.03006422637931539
layer1 relu: 0.03006422637931539
layer1 relu: 0.029033836503076327
SimAM: 0.029518178244839086
layer1 relu: 0.029518178244839086
layer2 relu: 0.038385860924128344
SimAM: 0.039987121655506906
layer2 relu: 0.039987121655506906
layer2 relu: 0.03809786938383813
SimAM: 0.040441179117101454
layer2 relu: 0.040441179117101454
layer2 relu: 0.03723951557470153
SimAM: 0.038968503339809446
layer2 relu: 0.038968503339809446
layer2 relu: 0.03738220518950914
SimAM: 0.03853622583692839
layer2 relu: 0.03853622583692839
layer3 relu: 0.055617484697285516
SimAM: 0.05461287873344566
layer3 relu: 0.05461287873344566
layer3 relu: 0.05360742355688106
SimAM: 0.05682916884477819
layer3 relu: 0.05682916884477819
layer3 relu: 0.05329486459410686
SimAM: 0.055064962107257476
layer3 relu: 0.055064962107257476
layer3 relu: 0.05488579639955431
SimAM: 0.05627188817123183
layer3 relu: 0.05627188817123183
layer3 relu: 0.053119042761081744
SimAM: 0.05596056996237541
layer3 relu: 0.05596056996237541
layer3 relu: 0.052315967563367684
SimAM: 0.05568634151330678
layer3 relu: 0.05568634151330678
layer4 relu: 0.0862836531034498
SimAM: 0.07747293010545048
layer4 relu: 0.07747293010545048
layer4 relu: 0.07756915976701068
SimAM: 0.07735234538061742
layer4 relu: 0.07735234538061742
layer4 relu: 0.08455390576878558
SimAM: 0.08057636707002867
layer4 relu: 0.08057636707002867
pooling: 0.3285071490695711
